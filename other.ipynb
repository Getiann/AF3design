{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class OptimizeSeq:\n",
    "    def __init__(self,featurised_examples, model_runner, fold_input,seed):\n",
    "        self.example = featurised_examples[0]\n",
    "        self.seed = fold_input.rng_seeds[0]\n",
    "        self.rng_key = jax.random.PRNGKey(seed)\n",
    "        self.logits, batch = model_runner.run_inference(self.example, self.rng_key)\n",
    "\n",
    "\n",
    "    def compute_stage_one(self, iteration,temperature=1.0):\n",
    "        \"\"\"\n",
    "        lambda = (step + 1) / iterations\n",
    "        temperature = 1.0\n",
    "        \"\"\"\n",
    "            # (1 - λ) * logits + λ * softmax(logits / temperature)\n",
    "        lambda_ = (step + 1) / self.iterations_1\n",
    "        self.logits = (1 - lambda_) * self.logits + lambda_ * jax.nn.softmax(self.logits / temperature)\n",
    "\n",
    "\n",
    "    def compute_stage_two(self, iteration, temperature_initial=1e-2):\n",
    "        \"\"\"\n",
    "        temperature = (1e-2 + (1 - 1e-2) * (1 - (step + 1) / iterations)^2)\n",
    "        \"\"\"\n",
    "        for step in range(iteration):\n",
    "            # temperature = (1e-2 + (1 - 1e-2) * (1 - (step + 1) / iterations)^2)\n",
    "            temperature = temperature_initial + (1 - temperature_initial) * (1 - (step + 1) / self.iterations_2)**2\n",
    "            self.step(lr=0.1, batch=None, seq_logits=self.logits)\n",
    "            self.logits = jax.nn.softmax(self.logits / temperature)\n",
    "\n",
    "\n",
    "    def get_final_sequence(self, softmax_logits):\n",
    "        # argmax(softmax_logits) and (softmax_logits - softmax_logits).stop_gradient + softmax_logits\n",
    "        one_hot = jax.nn.one_hot(jnp.argmax(softmax_logits), softmax_logits.shape[-1])\n",
    "        final_sequence = one_hot - softmax_logits\n",
    "        return final_sequence.stop_gradient + softmax_logits\n",
    "\n",
    "    def process(self, step):\n",
    "        stage_one_output = self.compute_stage_one(step)\n",
    "        stage_two_output = self.compute_stage_two(step)\n",
    "        final_softmax = self.get_final_sequence(stage_two_output)\n",
    "\n",
    "        return final_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRunner:\n",
    "  \"\"\"Helper class to run structure prediction stages.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      config: model.Model.Config,\n",
    "      device: jax.Device,\n",
    "      model_dir: pathlib.Path,\n",
    "  ):\n",
    "    self._model_config = config\n",
    "    self._device = device\n",
    "    self._model_dir = model_dir\n",
    "    self.o = optax.adam(1.0)\n",
    "\n",
    "  @functools.cached_property\n",
    "  def model_params(self) -> hk.Params:\n",
    "    \"\"\"Loads model parameters from the model directory.\"\"\"\n",
    "    return params.get_model_haiku_params(model_dir=self._model_dir)\n",
    "\n",
    "  # def debug_run(self)\n",
    "  #   @hk.transform\n",
    "  #   def forward_fn(batch):\n",
    "  #     return model.Model(self._model_config)(batch)\n",
    "\n",
    "  #   return functools.partial(\n",
    "  #     forward_fn.apply, self.model_params\n",
    "  #   )\n",
    "\n",
    "  @functools.cached_property\n",
    "  def _model(\n",
    "      self\n",
    "  ) -> Callable[[jnp.ndarray, features.BatchDict], model.ModelResult]:\n",
    "    \"\"\"Loads model parameters and returns a jitted model forward pass.\"\"\"\n",
    "\n",
    "    @hk.transform\n",
    "    def forward_fn(batch):\n",
    "        return model.Model(self._model_config)(batch)\n",
    "\n",
    "    return functools.partial(\n",
    "        jax.jit(forward_fn.apply, device=self._device), self.model_params # haiku apply 第一个参数是params 第二个参数是rng\n",
    "    )\n",
    "\n",
    "  @functools.cached_property\n",
    "  def _designmodel(\n",
    "      self\n",
    "  ) -> Callable[[jnp.ndarray, features.BatchDict], model.ModelResult]:\n",
    "    \"\"\"Loads model parameters and returns a jitted model forward pass.\"\"\"\n",
    "\n",
    "    @hk.transform\n",
    "    def forward_fn(seq_logits,batch):\n",
    "        # import pprint\n",
    "        # pprint.pprint(self.model_params.keys())\n",
    "        return model.Af3Design(self._model_config)(seq_logits,batch)\n",
    "\n",
    "    return functools.partial(\n",
    "        jax.jit(forward_fn.apply, device=self._device),\n",
    "        self.model_params\n",
    "    )\n",
    "\n",
    "\n",
    "  def get_inference_feature(\n",
    "      self, featurised_example: features.BatchDict, rng_key=jax.random.PRNGKey(42), af3design: bool = True\n",
    "  ) -> model.ModelResult:\n",
    "    \"\"\"Computes a forward pass of the model on a featurised example.\"\"\"\n",
    "    featurised_example = jax.device_put(\n",
    "        jax.tree_util.tree_map(\n",
    "            jnp.asarray, utils.remove_invalidly_typed_feats(featurised_example)\n",
    "        ),\n",
    "        self._device,\n",
    "    )\n",
    "    if af3design:\n",
    "        batch = feat_batch.Batch.from_data_dict(featurised_example)\n",
    "        seq_logits = jax.nn.one_hot(\n",
    "              batch.token_features.aatype,\n",
    "              residue_names.POLYMER_TYPES_NUM_WITH_UNKNOWN_AND_GAP,\n",
    "          )\n",
    "        return seq_logits,featurised_example\n",
    "\n",
    "    else:\n",
    "        result = self._model(rng_key, featurised_example)\n",
    "\n",
    "    # print(result['distogram']['bin_edges'].shape, result['distogram']['contact_probs'].shape) [63,] [256,256]\n",
    "\n",
    "        result = jax.tree.map(np.asarray, result)\n",
    "        result = jax.tree.map(\n",
    "            lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x,\n",
    "            result,\n",
    "        )\n",
    "        result = dict(result)\n",
    "        identifier = self.model_params['__meta__']['__identifier__'].tobytes()\n",
    "        result['__identifier__'] = identifier\n",
    "        return result\n",
    "\n",
    "  def diffusion_result(self, result):\n",
    "        result = jax.tree.map(np.asarray, result)\n",
    "        result = jax.tree.map(\n",
    "            lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x,\n",
    "            result,\n",
    "        )\n",
    "        result = dict(result)\n",
    "        identifier = self.model_params['__meta__']['__identifier__'].tobytes()\n",
    "        result['__identifier__'] = identifier\n",
    "        return result\n",
    "\n",
    "  def updata_seq(self, seq_logits, opt):\n",
    "      step = opt['step']\n",
    "      iteration = opt['iteration']\n",
    "      t = opt['t']\n",
    "      stage = opt['stage']\n",
    "\n",
    "      def stage_1_fn(seq_logits):\n",
    "          lambda_ = (step + 1) / iteration\n",
    "          return (1 - lambda_) * seq_logits + lambda_ * jax.nn.softmax(seq_logits / t)\n",
    "\n",
    "      def stage_2_fn(seq_logits):\n",
    "          temperature_initial = 1e-2\n",
    "          temperature = temperature_initial + (1 - temperature_initial) * (1 - (step + 1) / iteration)**2\n",
    "          return jax.nn.softmax(seq_logits / temperature)\n",
    "\n",
    "      def stage_3_fn(seq_logits):\n",
    "          softmax_logits = jax.nn.softmax(seq_logits)\n",
    "          final_sequence = jax.nn.one_hot(jnp.argmax(softmax_logits), softmax_logits.shape[-1]) - softmax_logits\n",
    "          return jax.lax.stop_gradient(final_sequence) + softmax_logits\n",
    "\n",
    "      seq_logits = lax.cond(\n",
    "          stage == 1,\n",
    "          stage_1_fn,\n",
    "          lambda _: lax.cond(\n",
    "              stage == 2,\n",
    "              stage_2_fn,\n",
    "              lambda _: stage_3_fn(seq_logits),\n",
    "              seq_logits\n",
    "          ),\n",
    "          seq_logits\n",
    "      )\n",
    "\n",
    "      return seq_logits\n",
    "\n",
    "\n",
    "  def get_model(self,example):\n",
    "      # forward pass\n",
    "      @remat\n",
    "      def _model(seq_logits, batch,rng):\n",
    "        # logits -> sequence representation\n",
    "        opt = batch['design_opt']\n",
    "        seq_logits = self.updata_seq(seq_logits,opt)\n",
    "        result=  self._designmodel(rng, seq_logits, batch)\n",
    "        plddt_loss = confidence_loss(result['predicted_lddt'], batch['is_ligand'], example)\n",
    "        dis_loss = contact_loss_dgram(result['distogram']['probs_logits'],\n",
    "          result['distogram']['bin_edges'],\n",
    "          batch['entity_id'])\n",
    "\n",
    "        loss = dis_loss + plddt_loss\n",
    "        return loss,result\n",
    "      return jax.value_and_grad(_model, argnums=0,has_aux=True)\n",
    "\n",
    "  def update_grad(self, grad, params, state):\n",
    "    updates, new_state = self.o.update(grad, state, params)\n",
    "    grad = jax.tree_util.tree_map(lambda x:-x, updates)\n",
    "    return new_state, grad\n",
    "\n",
    "seq_logits,batch = model_runner.get_inference_feature(example)\n",
    "state = model_runner.o.init(seq_logits)\n",
    "optimizer = jax.jit(model_runner.update_grad)\n",
    "grad_fn = model_runner.get_model(example)\n",
    "(loss, aux), grad= grad_fn(seq_logits, batch,jax.random.PRNGKey(42))\n",
    "aux = model_runner.diffusion_result(aux)\n",
    "state,grad = optimizer(grad, seq_logits, state)\n",
    "lr =  schedule(step)\n",
    "\n",
    "seq_logits = jax.tree_util.tree_map(lambda x,g:x-lr*g, seq_logits, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# 假设的 residue_chain 数组\n",
    "residue_chain = jnp.array([1, 1, 1, 0, 0])\n",
    "\n",
    "# 创建二维掩码\n",
    "mask = jnp.where(residue_chain[:, None] == 0, 0, 1.0)\n",
    "mask = mask * mask.swapaxes(0, 1)\n",
    "\n",
    "print(mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/ge/input/test.json'\n",
    "import json\n",
    "with open(json_path, 'r') as f:\n",
    "    json_str = f.read()\n",
    "\n",
    "# Parse the JSON string, so we can detect its format.\n",
    "raw_json = json.loads(json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
